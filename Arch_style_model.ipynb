{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Arch style model",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GastonBC/ArchClassifierML/blob/master/Arch_style_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHwhSH20BIeT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PLy3pthUS0D2",
        "colab": {}
      },
      "source": [
        "# This is to use local dataset\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/My Drive/Python Programs/datasets/arcDataset.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "# Files are extracted to /tmp/arcDataset\n",
        "\n",
        "base_dir = '/tmp/arcDataset'\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'Training')\n",
        "validation_dir = os.path.join(base_dir, 'Validation')\n",
        "\n",
        "# our training dir\n",
        "train_Achaemenid_architecture_dir = os.path.join(train_dir, 'Achaemenid architecture')\n",
        "train_American_craftsman_style_dir = os.path.join(train_dir, 'American craftsman style')\n",
        "train_American_Foursquare_architecture_dir = os.path.join(train_dir, 'American Foursquare architecture')\n",
        "train_Ancient_Egyptian_architecture_dir = os.path.join(train_dir, 'Ancient Egyptian architecture')\n",
        "train_Art_Deco_architecture_dir = os.path.join(train_dir, 'Art Deco architecture')\n",
        "train_Art_Nouveau_architecture_dir = os.path.join(train_dir, 'Art Nouveau architecture')\n",
        "train_Baroque_architecture_dir = os.path.join(train_dir, 'Baroque architecture')\n",
        "train_Bauhaus_architecture_dir = os.path.join(train_dir, 'Bauhaus architecture')\n",
        "train_Beaux_Arts_architecture_dir = os.path.join(train_dir, 'Beaux Arts architecture')\n",
        "train_Byzantine_architecture_dir = os.path.join(train_dir, 'Byzantine architecture')\n",
        "train_Chicago_school_architecture_dir = os.path.join(train_dir, 'Chicago school architecture')\n",
        "train_Colonial_architecture_dir = os.path.join(train_dir, 'Colonial architecture')\n",
        "train_Deconstructivism_dir = os.path.join(train_dir, 'Deconstructivism')\n",
        "train_Edwardian_architecture_dir = os.path.join(train_dir, 'Edwardian architecture')\n",
        "train_Georgian_architecture_dir = os.path.join(train_dir, 'Georgian architecture')\n",
        "train_Gothic_architecture_dir = os.path.join(train_dir, 'Gothic architecture')\n",
        "train_Greek_Revival_architecture_dir = os.path.join(train_dir, 'Greek Revival architecture')\n",
        "train_International_style_dir = os.path.join(train_dir, 'International style')\n",
        "train_Novelty_architecture_dir = os.path.join(train_dir, 'Novelty architecture')\n",
        "train_Palladian_architecture_dir = os.path.join(train_dir, 'Palladian architecture')\n",
        "train_Postmodern_architecture_dir = os.path.join(train_dir, 'Postmodern architecture')\n",
        "train_Queen_Anne_architecture_dir = os.path.join(train_dir, 'Queen Anne architecture')\n",
        "train_Romanesque_architecture_dir = os.path.join(train_dir, 'Romanesque architecture')\n",
        "train_Russian_Revival_architecture_dir = os.path.join(train_dir, 'Russian Revival architecture')\n",
        "train_Tudor_Revival_architecture_dir = os.path.join(train_dir, 'Tudor Revival architecture')\n",
        "\n",
        "\n",
        "# Our validation directory\n",
        "validation_Achaemenid_architecture_dir = os.path.join(train_dir, 'Achaemenid architecture')\n",
        "validation_American_craftsman_style_dir = os.path.join(train_dir, 'American craftsman style')\n",
        "validation_American_Foursquare_architecture_dir = os.path.join(train_dir, 'American Foursquare architecture')\n",
        "validation_Ancient_Egyptian_architecture_dir = os.path.join(train_dir, 'Ancient Egyptian architecture')\n",
        "validation_Art_Deco_architecture_dir = os.path.join(train_dir, 'Art Deco architecture')\n",
        "validation_Art_Nouveau_architecture_dir = os.path.join(train_dir, 'Art Nouveau architecture')\n",
        "validation_Baroque_architecture_dir = os.path.join(train_dir, 'Baroque architecture')\n",
        "validation_Bauhaus_architecture_dir = os.path.join(train_dir, 'Bauhaus architecture')\n",
        "validation_Beaux_Arts_architecture_dir = os.path.join(train_dir, 'Beaux-Arts architecture')\n",
        "validation_Byzantine_architecture_dir = os.path.join(train_dir, 'Byzantine architecture')\n",
        "validation_Chicago_school_architecture_dir = os.path.join(train_dir, 'Chicago school architecture')\n",
        "validation_Colonial_architecture_dir = os.path.join(train_dir, 'Colonial architecture')\n",
        "validation_Deconstructivism_dir = os.path.join(train_dir, 'Deconstructivism')\n",
        "validation_Edwardian_architecture_dir = os.path.join(train_dir, 'Edwardian architecture')\n",
        "validation_Georgian_architecture_dir = os.path.join(train_dir, 'Georgian architecture')\n",
        "validation_Gothic_architecture_dir = os.path.join(train_dir, 'Gothic architecture')\n",
        "validation_Greek_Revival_architecture_dir = os.path.join(train_dir, 'Greek Revival architecture')\n",
        "validation_International_style_dir = os.path.join(train_dir, 'International style')\n",
        "validation_Novelty_architecture_dir = os.path.join(train_dir, 'Novelty architecture')\n",
        "validation_Palladian_architecture_dir = os.path.join(train_dir, 'Palladian architecture')\n",
        "validation_Postmodern_architecture_dir = os.path.join(train_dir, 'Postmodern architecture')\n",
        "validation_Queen_Anne_architecture_dir = os.path.join(train_dir, 'Queen Anne architecture')\n",
        "validation_Romanesque_architecture_dir = os.path.join(train_dir, 'Romanesque architecture')\n",
        "validation_Russian_Revival_architecture_dir = os.path.join(train_dir, 'Russian Revival architecture')\n",
        "validation_Tudor_Revival_architecture_dir = os.path.join(train_dir, 'Tudor Revival architecture')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qvfZg3LQbD-5",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PixZ2s5QbYQ3",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(300, 300, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2), \n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(25, activation=\"softmax\")  \n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7ZKj8392nbgP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "6419aefd-0924-4368-809a-f256753e4be3"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 78400)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               40141312  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 25)                3225      \n",
            "=================================================================\n",
            "Total params: 40,233,785\n",
            "Trainable params: 40,233,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DmtkTn06pKxF"
      },
      "source": [
        "The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8DHWhFP_uhq3",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=\"RMSProp\",\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Sn9m9D3UimHM"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Let's set up data generators that will read pictures in our source folders, convert them to `float32` tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of 20 images of size 150x150 and their labels (binary).\n",
        "\n",
        "Data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
        "\n",
        "In Keras this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)` or `.flow_from_directory(directory)`. These generators can then be used with the Keras model methods that accept data generators as inputs: `fit`, `evaluate_generator`, and `predict_generator`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ClebU9NJg99G",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "45b511ae-002f-45cf-8234-683f7f9b344c"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255.\n",
        "train_datagen = ImageDataGenerator( rescale = 1.0/255.0,\n",
        "                                    rotation_range = 40,\n",
        "                                    width_shift_range = 0.2,\n",
        "                                    height_shift_range = 0.2,\n",
        "                                    shear_range = 0.2,\n",
        "                                    zoom_range = 0.2,\n",
        "                                    horizontal_flip = True,\n",
        "                                    fill_mode = 'nearest')\n",
        "test_datagen  = ImageDataGenerator( rescale = 1.0/255.0,\n",
        "                                    rotation_range = 40,\n",
        "                                    width_shift_range = 0.2,\n",
        "                                    height_shift_range = 0.2,\n",
        "                                    shear_range = 0.2,\n",
        "                                    zoom_range = 0.2,\n",
        "                                    horizontal_flip = True,\n",
        "                                    fill_mode = 'nearest')\n",
        "\n",
        "# --------------------\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "# --------------------\n",
        "train_generator = train_datagen.flow_from_directory(train_dir,\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode=\"categorical\",\n",
        "                                                    target_size=(300, 300))     \n",
        "# --------------------\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "# --------------------\n",
        "validation_generator =  test_datagen.flow_from_directory(validation_dir,\n",
        "                                                         batch_size=20,\n",
        "                                                         class_mode  = \"categorical\",\n",
        "                                                         target_size = (300, 300))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 3369 images belonging to 25 classes.\n",
            "Found 1425 images belonging to 25 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfeaxOHnvqK6",
        "colab_type": "text"
      },
      "source": [
        "### Model training in batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fb1_lgobv81m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "be546108-1105-4b93-c5bb-eef49bebee91"
      },
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    steps_per_epoch=169,\n",
        "                    epochs=10,\n",
        "                    validation_steps=72)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "169/169 [==============================] - 134s 794ms/step - loss: 3.3091 - accuracy: 0.1149 - val_loss: 3.1385 - val_accuracy: 0.0568\n",
            "Epoch 2/10\n",
            "169/169 [==============================] - 130s 770ms/step - loss: 2.8932 - accuracy: 0.1650 - val_loss: 2.8855 - val_accuracy: 0.1635\n",
            "Epoch 3/10\n",
            "169/169 [==============================] - 127s 751ms/step - loss: 2.7488 - accuracy: 0.1953 - val_loss: 2.9320 - val_accuracy: 0.1530\n",
            "Epoch 4/10\n",
            "169/169 [==============================] - 125s 740ms/step - loss: 2.6020 - accuracy: 0.2437 - val_loss: 3.1696 - val_accuracy: 0.1670\n",
            "Epoch 5/10\n",
            "169/169 [==============================] - 126s 748ms/step - loss: 2.5230 - accuracy: 0.2618 - val_loss: 2.7086 - val_accuracy: 0.2091\n",
            "Epoch 6/10\n",
            "169/169 [==============================] - 128s 757ms/step - loss: 2.4637 - accuracy: 0.2852 - val_loss: 2.8802 - val_accuracy: 0.2049\n",
            "Epoch 7/10\n",
            "169/169 [==============================] - 127s 750ms/step - loss: 2.4048 - accuracy: 0.2826 - val_loss: 2.9035 - val_accuracy: 0.2035\n",
            "Epoch 8/10\n",
            "169/169 [==============================] - 127s 753ms/step - loss: 2.3583 - accuracy: 0.3140 - val_loss: 3.2829 - val_accuracy: 0.1705\n",
            "Epoch 9/10\n",
            "169/169 [==============================] - 129s 762ms/step - loss: 2.3134 - accuracy: 0.3134 - val_loss: 3.1955 - val_accuracy: 0.2140\n",
            "Epoch 10/10\n",
            "169/169 [==============================] - 126s 748ms/step - loss: 2.2692 - accuracy: 0.3292 - val_loss: 2.6330 - val_accuracy: 0.2421\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "o6vSHzPR2ghH"
      },
      "source": [
        "###Running the Model\n",
        "\n",
        "This code will allow you to choose 1 or more files from your file system, it will then upload them, and run them through the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DoWp43WxJDNT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "95d8e3d3-631e-41b3-f864-b2d9cad16dd7"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded=files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path='/content/' + fn\n",
        "  img=image.load_img(path, target_size=(300, 300))\n",
        "  \n",
        "  x=image.img_to_array(img)\n",
        "  x=np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  \n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  idx = max(classes[0])\n",
        "  print(idx)\n",
        "  print(classes[0])\n",
        "  print(\"------------------\")\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-acabcbb6-a5b3-47d1-9121-3298763a919d\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-acabcbb6-a5b3-47d1-9121-3298763a919d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8336f342de31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0muploaded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploaded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m   result = _output.eval_js(\n\u001b[1;32m     63\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[0;32m---> 64\u001b[0;31m           input_id=input_id, output_id=output_id))\n\u001b[0m\u001b[1;32m     65\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    104\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    105\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: TypeError: Cannot read property '_uploadFiles' of undefined",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: Current TensorFlow version is 2.2.0-rc2. To use TF 1.x instead,\nrestart your runtime (Ctrl+M .) and run \"%tensorflow_version 1.x\" before\nyou run \"import tensorflow\".\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-8EHQyWGDvWz"
      },
      "source": [
        "### Visualizing Intermediate Representations\n",
        "\n",
        "To get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\n",
        "\n",
        "Let's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-5tES8rXFjux",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from   tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Parameters for our graph; we'll output images in a 4x4 configuration\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "pic_index = 0 # Index for iterating over images\n",
        "\n",
        "train_cat_fnames = os.listdir(train_Achaemenid_architecture_dir)\n",
        "train_dog_fnames = os.listdir(train_Greek_Revival_architecture_dir)\n",
        "\n",
        "# Let's define a new Model that will take an image as input, and will output\n",
        "# intermediate representations for all layers in the previous model after\n",
        "# the first.\n",
        "successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "\n",
        "#visualization_model = Model(img_input, successive_outputs)\n",
        "visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "\n",
        "# Let's prepare a random input image of a cat or dog from the training set.\n",
        "cat_img_files = [os.path.join(train_Achaemenid_architecture_dir, f) for f in train_cat_fnames]\n",
        "dog_img_files = [os.path.join(train_Greek_Revival_architecture_dir, f) for f in train_dog_fnames]\n",
        "\n",
        "img_path = random.choice(cat_img_files + dog_img_files)\n",
        "img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "\n",
        "x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
        "x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "# Rescale by 1/255\n",
        "x /= 255.0\n",
        "\n",
        "# Let's run our image through our network, thus obtaining all\n",
        "# intermediate representations for this image.\n",
        "successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "# These are the names of the layers, so can have them as part of our plot\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# Now let's display our representations\n",
        "# -----------------------------------------------------------------------\n",
        "for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "  \n",
        "  if len(feature_map.shape) == 4:\n",
        "    \n",
        "    #-------------------------------------------\n",
        "    # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "    #-------------------------------------------\n",
        "    n_features = feature_map.shape[-1]  # number of features in the feature map\n",
        "    size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
        "    \n",
        "    # We will tile our images in this matrix\n",
        "    display_grid = np.zeros((size, size * n_features))\n",
        "    \n",
        "    #-------------------------------------------------\n",
        "    # Postprocess the feature to be visually palatable\n",
        "    #-------------------------------------------------\n",
        "    for i in range(n_features):\n",
        "      x  = feature_map[0, :, :, i]\n",
        "      x -= x.mean()\n",
        "      x /= x.std ()\n",
        "      x *=  64\n",
        "      x += 128\n",
        "      x  = np.clip(x, 0, 255).astype('uint8')\n",
        "      display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
        "\n",
        "    #-----------------\n",
        "    # Display the grid\n",
        "    #-----------------\n",
        "\n",
        "    scale = 20. / n_features\n",
        "    plt.figure( figsize=(scale * n_features, scale) )\n",
        "    plt.title ( layer_name )\n",
        "    plt.grid  ( False )\n",
        "    plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q5Vulban4ZrD"
      },
      "source": [
        "### Evaluating Accuracy and Loss for the Model\n",
        "\n",
        "Pplot the training/validation accuracy and loss as collected during training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0oj0gTIy4k60",
        "colab": {}
      },
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc      = history.history[     'accuracy' ]\n",
        "val_acc  = history.history[ 'val_accuracy' ]\n",
        "loss     = history.history[    'loss' ]\n",
        "val_loss = history.history['val_loss' ]\n",
        "\n",
        "epochs   = range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     acc )\n",
        "plt.plot  ( epochs, val_acc )\n",
        "plt.title ('Training and validation accuracy')\n",
        "plt.figure()\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot  ( epochs,     loss )\n",
        "plt.plot  ( epochs, val_loss )\n",
        "plt.title ('Training and validation loss'   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "651IgjLyo-Jx",
        "colab": {}
      },
      "source": [
        "import os, signal\n",
        "\n",
        "os.kill(     os.getpid() , \n",
        "         signal.SIGKILL\n",
        "       )"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}